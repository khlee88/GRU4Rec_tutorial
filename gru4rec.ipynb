{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## set parameters\n",
    "#ROOT = 'PATH/TO/data/processed/'\n",
    "ROOT = '/home/khlee/git/recommendation/GRU4Rec_TensorFlow/data/processed/'\n",
    "DATA_TYPE = 'sample'\n",
    "PATH_TO_TRAIN = ROOT + 'rsc15_train_{}.txt'.format(DATA_TYPE)\n",
    "PATH_TO_TEST = ROOT + 'rsc15_test_{}.txt'.format(DATA_TYPE)\n",
    "checkpoint_dir = './checkpoint'\n",
    "if not os.path.exists(checkpoint_dir): os.mkdir(checkpoint_dir)\n",
    "        \n",
    "layers = 1\n",
    "rnn_size = 100\n",
    "batch_size = 50\n",
    "drop_keep_prob = 0.7\n",
    "\n",
    "n_epochs = 3\n",
    "learning_rate = 0.001\n",
    "decay = 0.96\n",
    "decay_steps = 1e4\n",
    "grad_cap = 0\n",
    "print_step = 1e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## load data\n",
    "data = pd.read_csv(PATH_TO_TRAIN, sep='\\t', dtype={'ItemId': np.int64})\n",
    "valid = pd.read_csv(PATH_TO_TEST, sep='\\t', dtype={'ItemId': np.int64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "## check sort data\n",
    "### preprocessing에서 session, timestamp로 sorting을 하였는데,\n",
    "### sorting이 중요하기 때문에 한번더 확인해본다.\n",
    "### data가 클경우 sort에서 시간이 오래 걸리기 때문에 sample check을 수행한다.\n",
    "def check_data_sort(dt, sample_check=False, sample_size=10000):\n",
    "    if sample_check:\n",
    "        sess_ids = dt['SessionId'].unique()\n",
    "        sample_sess_ids = np.random.choice(sess_ids, sample_size, replace=False)\n",
    "        dt = dt[np.in1d(dt.SessionId, sample_sess_ids)]\n",
    "    ordered_dt = dt.sort_values(['SessionId', 'timestamp'])\n",
    "    return dt.equals(ordered_dt)\n",
    "\n",
    "print(check_data_sort(data, sample_check=True))\n",
    "print(check_data_sort(valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.58 s, sys: 96 ms, total: 1.68 s\n",
      "Wall time: 1.68 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SessionId</th>\n",
       "      <th>ItemId</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>ItemIdx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>214701242</td>\n",
       "      <td>1.396804e+09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>214826623</td>\n",
       "      <td>1.396804e+09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>214838503</td>\n",
       "      <td>1.396861e+09</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>214838503</td>\n",
       "      <td>1.396861e+09</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>214838503</td>\n",
       "      <td>1.396861e+09</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SessionId     ItemId     timestamp  ItemIdx\n",
       "0          6  214701242  1.396804e+09        0\n",
       "1          6  214826623  1.396804e+09        1\n",
       "2         21  214838503  1.396861e+09        2\n",
       "3         21  214838503  1.396861e+09        2\n",
       "4         21  214838503  1.396861e+09        2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## add item index \n",
    "### item id를 0번부터 index를 추가합니다.\n",
    "itemids = data['ItemId'].unique()\n",
    "n_items = len(itemids)\n",
    "itemidmap = pd.Series(data=np.arange(n_items), index=itemids).to_dict()\n",
    "%time data['ItemIdx'] = data['ItemId'].map(lambda x: itemidmap[x])\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  2,  8, 10, 15], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## offset sessions\n",
    "### 각 세션의 시작점의 index list를 만든다.\n",
    "### 즉, 첫번째 sessionid 6의 시작점은 0이고 21의 시작점은 2 이다.\n",
    "offset_sessions = np.zeros(data['SessionId'].nunique()+1, dtype=np.int32)\n",
    "offset_sessions[1:] = data.groupby('SessionId').size().cumsum()\n",
    "offset_sessions[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## placeholder & learning rate\n",
    "X = tf.placeholder(tf.int32, [batch_size], name='input')\n",
    "Y = tf.placeholder(tf.int32, [batch_size], name='output')\n",
    "States = [tf.placeholder(tf.float32, [batch_size, rnn_size], name='rnn_state') for _ in range(layers)]\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "lr = tf.maximum(1e-5,tf.train.exponential_decay(\n",
    "    learning_rate, global_step, decay_steps, decay, staircase=True\n",
    ")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## gru weigths\n",
    "### input item에 대한 embedding matrix 와\n",
    "### next item 즉 output을 위한 softmax W, b matrix를 구성한다.\n",
    "with tf.variable_scope('gru_layer', reuse=tf.AUTO_REUSE):\n",
    "    #sigma = sigma if sigma != 0 else np.sqrt(6.0 / (n_items + rnn_size))\n",
    "    #initializer = tf.random_uniform_initializer(minval=-sigma, maxval=sigma)\n",
    "    initializer = tf.glorot_uniform_initializer()\n",
    "    embedding = tf.get_variable('embedding', [n_items, rnn_size], initializer=initializer)\n",
    "    softmax_W = tf.get_variable('softmax_w', [n_items, rnn_size], initializer=initializer)\n",
    "    softmax_b = tf.get_variable('softmax_b', [n_items], initializer=tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## gru_cell\n",
    "### ㅁt => ㅁt+1 => ㅁt+2 => ... \n",
    "### 위와 같은 recurrent network에서 ㅁ. 즉, 단일 gru cell을 말한다.\n",
    "with tf.variable_scope('gru_cell', reuse=tf.AUTO_REUSE):\n",
    "    cell = tf.nn.rnn_cell.GRUCell(rnn_size, activation=tf.nn.tanh)\n",
    "    drop_cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=drop_keep_prob)\n",
    "    stacked_cell = tf.nn.rnn_cell.MultiRNNCell([drop_cell] * layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## feedforward gur_cell\n",
    "### 예를들어 seesion 1의 item sequence가 5, 7, 9 라면,\n",
    "### 첫번째 배치에서 아이템 5에 대하여 embedding 하여 inputs를 추출하고,\n",
    "### session 1의 초기 states로 부터 output과 final_state를 계산한다.\n",
    "### 두번째 배치에서는 아이템 7에 대한 inputs이 들어가고 아이템 5로 부터 계산된\n",
    "### final state로 아이템 7에 대한 output과 final_state가 다시 계산된다.\n",
    "### 즉, 배치 순서로 각 seesion의 item sequence가 recurrent하게 학습되는 것이다.\n",
    "inputs = tf.nn.embedding_lookup(embedding, X)\n",
    "output, state_ = stacked_cell(inputs, tuple(States))\n",
    "final_state = state_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## calculate cost(loss)\n",
    "### 학습일 경우 negative sampling을 통해 \n",
    "### cross-entropy loss로 계산하였다. bpt, top1 loss는 주석처리 하였다. \n",
    "\n",
    "### for training\n",
    "sampled_W = tf.nn.embedding_lookup(softmax_W, Y)\n",
    "sampled_b = tf.nn.embedding_lookup(softmax_b, Y)\n",
    "logits = tf.matmul(output, sampled_W, transpose_b=True) + sampled_b\n",
    "### cross-entropy loss\n",
    "yhat = tf.nn.softmax(logits)\n",
    "cost = tf.reduce_mean(-tf.log(tf.diag_part(yhat)+1e-24))\n",
    "### bpr loss\n",
    "# yhat = logits\n",
    "# yhatT = tf.transpose(yhat)\n",
    "# cost = tf.reduce_mean(-tf.log(tf.nn.sigmoid(tf.diag_part(yhat)-yhatT)))\n",
    "### top1 loss\n",
    "# yhat = logits\n",
    "# yhatT = tf.transpose(yhat)\n",
    "# term1 = tf.reduce_mean(tf.nn.sigmoid(-tf.diag_part(yhat)+yhatT)+tf.nn.sigmoid(yhatT**2), axis=0)\n",
    "# term2 = tf.nn.sigmoid(tf.diag_part(yhat)**2) / batch_size\n",
    "# cost = tf.reduce_mean(term1 - term2)\n",
    "\n",
    "### for prediction\n",
    "logits_all = tf.matmul(output, softmax_W, transpose_b=True) + softmax_b\n",
    "yhat_all = tf.nn.softmax(logits_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### understanding negative sampling & loss\n",
    "#### Negative sampling \n",
    "위 코드에서 Negative sampling이 어떻게 되는 것인지 이해해보자. <br>\n",
    "Y 즉, 정답(label) item에 대한 softmax weight 만 추출하여 logit 및 yhat을 계산하고 loss를 계산하는데,\n",
    "이는 배치 하나의 정답셋(아이템)에 대하여 나머지 item(49개)을 negative로 두어 loss를 계산하는 방식이다.\n",
    "(나머지 item 중 정답셋과 중복이 되면 어떻게 되나???)\n",
    "예를 들어 50개의 배치에 rnn out size가 20이라면, output의 shape은 (50, 20)이 될것이다.\n",
    "sampled_W, sampled_b는 각각 (50, 20), (50,)가 될 것이다. 그리고 output shape은 (50, 50)이 된다.\n",
    "마지막으로 각 row별 softmax를 취해 yhat을 계산한다.\n",
    "하나의 row(배치 하나)를 봤을 때, [n, n] 위치가 해당 배치의 정답 item의 yhat 값이 된다.\n",
    "\n",
    "#### Ranking Loss \n",
    "softmax cross-entropy loss는 위와 같이 logits값을 softmax한 후 정답 item의 값에 대해 위와같이 간단하게 계산할 수 있다.\n",
    "(단점으로 negative item 중 positive item과 중복이 발생할 수 있는데 이는 loss를 크게 만들 것으로 보인다.)\n",
    "논문에 나오는 BPR loss와 TOP1 loss를 알아보자.\n",
    "\n",
    "BPR loss <br>\n",
    "$L_s = - \\frac{1}{N_s} \\cdot \\sum_{j=1}^{N_s} \\text{log}(\\sigma(\\hat{r}_{s,i} -\\hat{r}_{s,j}))$ <br>\n",
    "$N_s$: sample size，$\\hat{r}_{s,i}$: yhat of positive sample， $\\hat{r}_{s,j}$: yhat of positive sample\n",
    "\n",
    "postive - negative값에 시그모이드 한 값을 사용하였다. <br>\n",
    "의미론적으로 보면, pos - neg 값이 클수록 잘 예측된 것이고, 이는 sigmoid 후 1에 가까워 진다. 즉, log변환을 통해 0에 가까워 지므로 적은 loss를 갖는다. 반대로 pos - neg 값이 음수이면 잘 못 예측한 것이고 결국 loss가 커지게 되는 것이다. <br>\n",
    "코드로 이해하면, tf.diag_part는 logits의 대각행렬 즉, pos 값을 추출한 것이고 shape은 (50,)가 된다. <br>\n",
    "neg 값과 차를 계산하기 위해 logits 값을 transpose하여 차를 계산하고 sigmoid후 loss를 계산하는 것이다. <br>\n",
    "\n",
    "TOP loss <br>\n",
    "$L_s = \\frac{1}{N_s} \\cdot \\sum_{j=1}^{N_s} (\\sigma(\\hat{r}_{s,j} - \\hat{r}_{s,i})) +\\sigma(\\hat{r^2_j})$ <br>\n",
    "\n",
    "log를 loss를 빼고, bpr과 반대로 neg - pos 값을 사용하였다. <br>\n",
    "의미론 적으로, 잘 예측 했다면 pos는 크고 neg는 작기 때문에 음의 값이 될 것이다. sigmoid드 후 0에 가까워 질 것 이다. 즉 첫번째 loss 텀이 0에 가까워 질 것이다. 값을 잘 예측하지 못한다면 양의 값이 될 것이고 loss 텀이 1에 가까워 질 것이다. <br>\n",
    "만약 pos값을 크게 예측하였다. 하지만, neg값 역시 크게 예측되었다면, 좋은 예측은 아닐 것이다. neg 값은 작아져야 하기 때문에, 이러한 경우에 오른쪽 정규화 텀으로부터 높은 neg값에 penalty를 부여하는 것이다. <br>\n",
    "위 코드에서 term1이 위와 같은 것이고, term2는 저자 코드를 확인해봐도 사용하고 있는데 계정판에서 추가된 것으로 보인다. <br> term1에서도 정규화 텀에서도 yhatT의 값을 전부 사용했는데, 큰차이가 없어서 그런 것인지? 명확하게는 대각행렬인 pos 값은 제외해야 하는 것이 아닌지 생각이 든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## optimize\n",
    "### Adam optimizer를 사용한다.\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "### grad_cap>0 다면, minimize시 gradient cliping을 수행한다.\n",
    "### gradient cliping을 수행하는 이유는 다음 블로그 참조 (https://dhhwang89.tistory.com/90)\n",
    "### 간략하게 학습 중에 gradient가 급격하게 변하는 지점이 발생할 수 있는데, 이는 기존 minima를 찾아가는 방향이 \n",
    "### 급변할 수 있기 때문에, 이를 방지하기 위해 수행한다.\n",
    "### 본 학습에서는 cliping을 하지 않는데, 유사하게 learning rate decay을 사용하기 때문인 것으로 생각됨.\n",
    "tvars = tf.trainable_variables()\n",
    "gvs = optimizer.compute_gradients(cost, tvars)\n",
    "if grad_cap > 0:\n",
    "    capped_gvs = [(tf.clip_by_norm(grad, grad_cap), var) for grad, var in gvs]\n",
    "else:\n",
    "    capped_gvs = gvs \n",
    "train_op = optimizer.apply_gradients(capped_gvs, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## session start\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(tf.global_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### understanding data feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4] ... [45 46 47 48 49]\n",
      "49\n",
      "[ 0  2  8 10 15]\n",
      "[ 2  8 10 15 19]\n"
     ]
    }
   ],
   "source": [
    "### 1. 초기 세팅으로 batch_size 만큼 index array를 만들고 maxiter값을 저장한다.\n",
    "### 2. start는 offset_session(sessionid의 시작 index) 에서 iters를 추출한다.\n",
    "###    즉, 첫 50개 sessionid의 시작 index를 추출다.\n",
    "### 3. end는 각 세션에서 다음 세션의 시작되는 index를 추출한다.\n",
    "iters = np.arange(batch_size)\n",
    "maxiter = iters.max()\n",
    "print(iters[:5], \"...\", iters[-5:])\n",
    "print(maxiter)\n",
    "start = offset_sessions[iters]\n",
    "end = offset_sessions[iters+1]\n",
    "print(start[:5])\n",
    "print(end[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[0 2 4 6 7]\n"
     ]
    }
   ],
   "source": [
    "### 1. end - start의 최소 값을 추출한다.\n",
    "### 만약 최소값이 3라면 즉, 하나의 세션의 item이 두개라면,\n",
    "### 첫번째 item은 input으로 사용되고 두번째 item은 output으로 사용된다.\n",
    "### 그리고 다음 배체에서 두번째 item은 input으로 사용되고 세번째 item이 output으로 사용된다.\n",
    "### 해당 세션은 배치가 두번 돈 후 더이상 학습 할 수 없으므로 다음 세션으로 교체되어야 한다.\n",
    "### 만약 최소값으 2라면, 해당 세션은 1번 배치 후 다음 세션으로 교체되어야 한다.\n",
    "### 즉, end - start - 1의 최소 값은 현재 배치된 session의 반복 수를 의미한다.\n",
    "### 2. out_idx는 각 session의 첫 itemidx를 나타낸다.\n",
    "minlen = (end-start).min()\n",
    "out_idx = data.ItemIdx.values[start]\n",
    "print(minlen)\n",
    "print(out_idx[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 4 6 7]\n",
      "[1 2 5 6 8]\n"
     ]
    }
   ],
   "source": [
    "### 위에서 설명한 것과 같이, 각 세션의 첫번째 아이템이 in, 두번째 아이템이 out이 된 후 학습에 사용된다.\n",
    "### minlen - 1 의 수만큼 반복(i)되어 학습한다.\n",
    "i = 0\n",
    "in_idx = out_idx\n",
    "out_idx = data.ItemIdx.values[start+i+1]\n",
    "print(in_idx[:5])\n",
    "print(out_idx[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  8 10 15 19]\n",
      "[ 1  3  9 11 16]\n",
      "[ 0  2  8  9 10]\n"
     ]
    }
   ],
   "source": [
    "### 1. 위에서 minlen - 1만큼 반복이 완료 되었다는 것은, 교체되어야 할 sessionid가 있다는 말이다.\n",
    "### start 즉, 세션의 시작 index에서 minlen-1 만큼 증가(반복) 후에 end-start가 1보다 작다는 것은\n",
    "### 해당 세션의 item이 모두 사용되었다는 것이다. \n",
    "### 2. 해당 index를 mask에 저장한다.\n",
    "### 즉, 배치 index 0(2-1), 2(10-9),... 에 위치한 session의 item은 모두 사용된 것이다.\n",
    "start = start+minlen-1\n",
    "mask = np.arange(len(iters))[(end-start)<=1]\n",
    "print(end[:5])\n",
    "print(start[:5])\n",
    "print(mask[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50  1 51  3  4  5  6  7  8  9]\n",
      "[186   3 188  11  16  20  25  28  33  35]\n",
      "[188   8 191  15  19  24  27  32  34  36]\n"
     ]
    }
   ],
   "source": [
    "### 1. iters 즉, 배치 index의 0번에 maxiter(49) + 1인 다음 session index로 교체한다.\n",
    "### 마찬가지고 배치 index 2번에는 그다음 session index 51로 교체한다.\n",
    "iters[0] = 50\n",
    "iters[2] = 51\n",
    "print(iters[:10])\n",
    "### 2. start와 end는 각 데이터에서 각 세션id의 시작 index와 다음 세션의 시작 index 의미하였다.\n",
    "### 교체된 session 지점에 새로운 세션id에 대한 시작 index와 다음 세션의 시작 index로 교체한다.\n",
    "start[0] = offset_sessions[50]\n",
    "end[0] = offset_sessions[50+1]\n",
    "start[2] = offset_sessions[51]\n",
    "end[2] = offset_sessions[51+1]\n",
    "print(start[:10])\n",
    "print(end[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[186   3 188  11  16  20  25  28 191 195]\n",
      "[188   8 191  15  19  24  27  32 195 198]\n"
     ]
    }
   ],
   "source": [
    "### 위 두 단락에서 session item이 다 사용된 iter 0, 2번 index에 대해서만 교체하였는데,\n",
    "### 모든 교체되어야할 index 즉, 모든 mask에 대하여 session 교체를 수행한다.\n",
    "for idx in mask:\n",
    "    maxiter += 1\n",
    "    if maxiter >= len(offset_sessions)-1:\n",
    "        finished = True\n",
    "        break\n",
    "    iters[idx] = maxiter\n",
    "    start[idx] = offset_sessions[maxiter]\n",
    "    end[idx] = offset_sessions[maxiter+1]\n",
    "print(start[:10])\n",
    "print(end[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 새로운 session으로 교체되었기 때문에, rnn 학습의 해당 위치의 초기값 state 값도 0으로 초기화해준다.\n",
    "#if len(mask):\n",
    "#    for i in range(layers):\n",
    "#        state[i][mask] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3140308\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SessionId</th>\n",
       "      <th>ItemId</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>ItemIdx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3140305</th>\n",
       "      <td>11562131</td>\n",
       "      <td>214854542</td>\n",
       "      <td>1.411823e+09</td>\n",
       "      <td>21584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3140306</th>\n",
       "      <td>11562151</td>\n",
       "      <td>214536296</td>\n",
       "      <td>1.411769e+09</td>\n",
       "      <td>3483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3140307</th>\n",
       "      <td>11562151</td>\n",
       "      <td>214536296</td>\n",
       "      <td>1.411769e+09</td>\n",
       "      <td>3483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3140308</th>\n",
       "      <td>11562157</td>\n",
       "      <td>214580372</td>\n",
       "      <td>1.411648e+09</td>\n",
       "      <td>1881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3140309</th>\n",
       "      <td>11562157</td>\n",
       "      <td>214516012</td>\n",
       "      <td>1.411648e+09</td>\n",
       "      <td>1880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         SessionId     ItemId     timestamp  ItemIdx\n",
       "3140305   11562131  214854542  1.411823e+09    21584\n",
       "3140306   11562151  214536296  1.411769e+09     3483\n",
       "3140307   11562151  214536296  1.411769e+09     3483\n",
       "3140308   11562157  214580372  1.411648e+09     1881\n",
       "3140309   11562157  214516012  1.411648e+09     1880"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 전체 과정 (한번의 epoch)을 거치면, 마지막 sessionid 의 index 3140308에서 종료된다.\n",
    "finished = False\n",
    "endpoint_count = 0\n",
    "while not finished:\n",
    "    minlen = (end-start).min()\n",
    "    out_idx = data.ItemIdx.values[start]\n",
    "    for i in range(minlen-1):\n",
    "        in_idx = out_idx\n",
    "        out_idx = data.ItemIdx.values[start+i+1]\n",
    "        endpoint_count += len(out_idx)\n",
    "    \n",
    "    start = start+minlen-1\n",
    "    mask = np.arange(len(iters))[(end-start)<=1]\n",
    "\n",
    "    for idx in mask:\n",
    "        maxiter += 1\n",
    "        if maxiter >= len(offset_sessions)-1:\n",
    "            finished = True\n",
    "            break\n",
    "        iters[idx] = maxiter\n",
    "        start[idx] = offset_sessions[maxiter]\n",
    "        end[idx] = offset_sessions[maxiter+1]\n",
    "        \n",
    "print(max(start))\n",
    "data[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2348300\n",
      "2348603\n"
     ]
    }
   ],
   "source": [
    "### SessionId 별 size -1의 합의 전체 학습할 수 있는 데이터인데,\n",
    "### 위 data feed logic에서는 그보다 적게 끝난다.\n",
    "### batch size 만큼 session placeholder를 만들고 session의 item이 다 소진되면,\n",
    "### 다름 세션으로 교체하는 방식인데, 마지막 세션이 교체된 후 minlen 까지만 학습하게 된다.\n",
    "### 예를들어 마지막 50개 세션에서 49개가 item이 3개가 있고 1개가 2개의 item을 가지고 있다면,\n",
    "### minlen은 1이 되어 한번 학습 후, 나머지 49개의 세션에는 아직 학습할 item이 남아있지만, \n",
    "### 한개의 빈자리에 더이상 교체될 세션이 없어지므로 학습을 종료하게 되기 때문에 enpoint count가 적게 나타난다.\n",
    "### 예측에서는 위와 조금 다른 방식으로 logic을 바꿔 모든 데이터를 feed할 것이다.\n",
    "### (학습에서도 모든 데이터를 feed하는 방식으로 바꾸어도 무방할 것으로 보인다.)\n",
    "print(endpoint_count)\n",
    "print(sum(data.groupby('SessionId').size() - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\tStep 1\tlr: 0.00100\tloss: 3.9120\tElapsed: 0.2\n",
      "Epoch 0\tStep 1000\tlr: 0.00100\tloss: 3.3881\tElapsed: 16.2\n",
      "Epoch 0\tStep 2000\tlr: 0.00100\tloss: 3.0291\tElapsed: 32.1\n",
      "Epoch 0\tStep 3000\tlr: 0.00100\tloss: 2.8503\tElapsed: 48.0\n",
      "Epoch 0\tStep 4000\tlr: 0.00100\tloss: 2.7016\tElapsed: 63.7\n",
      "Epoch 0\tStep 5000\tlr: 0.00100\tloss: 2.6081\tElapsed: 79.5\n",
      "Epoch 0\tStep 6000\tlr: 0.00100\tloss: 2.5248\tElapsed: 95.2\n",
      "Epoch 0\tStep 7000\tlr: 0.00100\tloss: 2.4712\tElapsed: 110.9\n",
      "Epoch 0\tStep 8000\tlr: 0.00100\tloss: 2.4129\tElapsed: 126.7\n",
      "Epoch 0\tStep 9000\tlr: 0.00100\tloss: 2.3749\tElapsed: 142.5\n",
      "Epoch 0\tStep 10000\tlr: 0.00100\tloss: 2.3345\tElapsed: 158.6\n",
      "Epoch 0\tStep 11000\tlr: 0.00096\tloss: 2.3112\tElapsed: 174.7\n",
      "Epoch 0\tStep 12000\tlr: 0.00096\tloss: 2.2795\tElapsed: 190.7\n",
      "Epoch 0\tStep 13000\tlr: 0.00096\tloss: 2.2579\tElapsed: 206.5\n",
      "Epoch 0\tStep 14000\tlr: 0.00096\tloss: 2.2328\tElapsed: 222.2\n",
      "Epoch 0\tStep 15000\tlr: 0.00096\tloss: 2.2094\tElapsed: 238.0\n",
      "Epoch 0\tStep 16000\tlr: 0.00096\tloss: 2.1834\tElapsed: 253.9\n",
      "Epoch 0\tStep 17000\tlr: 0.00096\tloss: 2.1680\tElapsed: 269.7\n",
      "Epoch 0\tStep 18000\tlr: 0.00096\tloss: 2.1503\tElapsed: 285.5\n",
      "Epoch 0\tStep 19000\tlr: 0.00096\tloss: 2.1326\tElapsed: 301.4\n",
      "Epoch 0\tStep 20000\tlr: 0.00096\tloss: 2.1203\tElapsed: 317.3\n",
      "Epoch 0\tStep 21000\tlr: 0.00092\tloss: 2.1069\tElapsed: 333.2\n",
      "Epoch 0\tStep 22000\tlr: 0.00092\tloss: 2.0969\tElapsed: 349.0\n",
      "Epoch 0\tStep 23000\tlr: 0.00092\tloss: 2.0859\tElapsed: 364.8\n",
      "Epoch 0\tStep 24000\tlr: 0.00092\tloss: 2.0770\tElapsed: 380.7\n",
      "Epoch 0\tStep 25000\tlr: 0.00092\tloss: 2.0713\tElapsed: 396.5\n",
      "Epoch 0\tStep 26000\tlr: 0.00092\tloss: 2.0653\tElapsed: 412.3\n",
      "Epoch 0\tStep 27000\tlr: 0.00092\tloss: 2.0559\tElapsed: 428.1\n",
      "Epoch 0\tStep 28000\tlr: 0.00092\tloss: 2.0478\tElapsed: 443.9\n",
      "Epoch 0\tStep 29000\tlr: 0.00092\tloss: 2.0404\tElapsed: 459.7\n",
      "Epoch 0\tStep 30000\tlr: 0.00092\tloss: 2.0302\tElapsed: 475.4\n",
      "Epoch 0\tStep 31000\tlr: 0.00088\tloss: 2.0218\tElapsed: 491.2\n",
      "Epoch 0\tStep 32000\tlr: 0.00088\tloss: 2.0146\tElapsed: 507.0\n",
      "Epoch 0\tStep 33000\tlr: 0.00088\tloss: 2.0075\tElapsed: 522.8\n",
      "Epoch 0\tStep 34000\tlr: 0.00088\tloss: 2.0022\tElapsed: 538.6\n",
      "Epoch 0\tStep 35000\tlr: 0.00088\tloss: 1.9963\tElapsed: 554.4\n",
      "Epoch 0\tStep 36000\tlr: 0.00088\tloss: 1.9900\tElapsed: 570.2\n",
      "Epoch 0\tStep 37000\tlr: 0.00088\tloss: 1.9850\tElapsed: 586.0\n",
      "Epoch 0\tStep 38000\tlr: 0.00088\tloss: 1.9780\tElapsed: 601.8\n",
      "Epoch 0\tStep 39000\tlr: 0.00088\tloss: 1.9744\tElapsed: 617.6\n",
      "Epoch 0\tStep 40000\tlr: 0.00088\tloss: 1.9686\tElapsed: 633.4\n",
      "Epoch 0\tStep 41000\tlr: 0.00085\tloss: 1.9678\tElapsed: 649.2\n",
      "Epoch 0\tStep 42000\tlr: 0.00085\tloss: 1.9652\tElapsed: 665.0\n",
      "Epoch 0\tStep 43000\tlr: 0.00085\tloss: 1.9641\tElapsed: 680.8\n",
      "Epoch 0\tStep 44000\tlr: 0.00085\tloss: 1.9613\tElapsed: 696.6\n",
      "Epoch 0\tStep 45000\tlr: 0.00085\tloss: 1.9573\tElapsed: 712.4\n",
      "Epoch 0\tStep 46000\tlr: 0.00085\tloss: 1.9514\tElapsed: 728.2\n",
      "Epoch 1\tStep 47000\tlr: 0.00085\tloss: 1.9738\tElapsed: 744.2\n",
      "Epoch 1\tStep 48000\tlr: 0.00085\tloss: 1.7047\tElapsed: 760.0\n",
      "Epoch 1\tStep 49000\tlr: 0.00085\tloss: 1.6346\tElapsed: 775.8\n",
      "Epoch 1\tStep 50000\tlr: 0.00085\tloss: 1.5989\tElapsed: 791.6\n",
      "Epoch 1\tStep 51000\tlr: 0.00082\tloss: 1.5647\tElapsed: 807.4\n",
      "Epoch 1\tStep 52000\tlr: 0.00082\tloss: 1.5384\tElapsed: 823.2\n",
      "Epoch 1\tStep 53000\tlr: 0.00082\tloss: 1.5136\tElapsed: 839.0\n",
      "Epoch 1\tStep 54000\tlr: 0.00082\tloss: 1.5216\tElapsed: 854.9\n",
      "Epoch 1\tStep 55000\tlr: 0.00082\tloss: 1.5264\tElapsed: 870.7\n",
      "Epoch 1\tStep 56000\tlr: 0.00082\tloss: 1.5327\tElapsed: 886.6\n",
      "Epoch 1\tStep 57000\tlr: 0.00082\tloss: 1.5317\tElapsed: 902.4\n",
      "Epoch 1\tStep 58000\tlr: 0.00082\tloss: 1.5460\tElapsed: 918.2\n",
      "Epoch 1\tStep 59000\tlr: 0.00082\tloss: 1.5498\tElapsed: 934.0\n",
      "Epoch 1\tStep 60000\tlr: 0.00082\tloss: 1.5551\tElapsed: 949.8\n",
      "Epoch 1\tStep 61000\tlr: 0.00078\tloss: 1.5562\tElapsed: 965.6\n",
      "Epoch 1\tStep 62000\tlr: 0.00078\tloss: 1.5523\tElapsed: 981.4\n",
      "Epoch 1\tStep 63000\tlr: 0.00078\tloss: 1.5473\tElapsed: 997.3\n",
      "Epoch 1\tStep 64000\tlr: 0.00078\tloss: 1.5497\tElapsed: 1013.2\n",
      "Epoch 1\tStep 65000\tlr: 0.00078\tloss: 1.5496\tElapsed: 1029.0\n",
      "Epoch 1\tStep 66000\tlr: 0.00078\tloss: 1.5486\tElapsed: 1044.8\n",
      "Epoch 1\tStep 67000\tlr: 0.00078\tloss: 1.5531\tElapsed: 1060.7\n",
      "Epoch 1\tStep 68000\tlr: 0.00078\tloss: 1.5562\tElapsed: 1076.5\n",
      "Epoch 1\tStep 69000\tlr: 0.00078\tloss: 1.5589\tElapsed: 1092.3\n",
      "Epoch 1\tStep 70000\tlr: 0.00078\tloss: 1.5596\tElapsed: 1108.0\n",
      "Epoch 1\tStep 71000\tlr: 0.00075\tloss: 1.5613\tElapsed: 1123.8\n",
      "Epoch 1\tStep 72000\tlr: 0.00075\tloss: 1.5653\tElapsed: 1139.5\n",
      "Epoch 1\tStep 73000\tlr: 0.00075\tloss: 1.5675\tElapsed: 1155.3\n",
      "Epoch 1\tStep 74000\tlr: 0.00075\tloss: 1.5677\tElapsed: 1171.1\n",
      "Epoch 1\tStep 75000\tlr: 0.00075\tloss: 1.5679\tElapsed: 1186.9\n",
      "Epoch 1\tStep 76000\tlr: 0.00075\tloss: 1.5679\tElapsed: 1202.6\n",
      "Epoch 1\tStep 77000\tlr: 0.00075\tloss: 1.5662\tElapsed: 1218.3\n",
      "Epoch 1\tStep 78000\tlr: 0.00075\tloss: 1.5632\tElapsed: 1234.1\n",
      "Epoch 1\tStep 79000\tlr: 0.00075\tloss: 1.5628\tElapsed: 1249.8\n",
      "Epoch 1\tStep 80000\tlr: 0.00075\tloss: 1.5635\tElapsed: 1265.6\n",
      "Epoch 1\tStep 81000\tlr: 0.00072\tloss: 1.5651\tElapsed: 1281.3\n",
      "Epoch 1\tStep 82000\tlr: 0.00072\tloss: 1.5663\tElapsed: 1297.1\n",
      "Epoch 1\tStep 83000\tlr: 0.00072\tloss: 1.5673\tElapsed: 1312.9\n",
      "Epoch 1\tStep 84000\tlr: 0.00072\tloss: 1.5685\tElapsed: 1328.7\n",
      "Epoch 1\tStep 85000\tlr: 0.00072\tloss: 1.5687\tElapsed: 1344.4\n",
      "Epoch 1\tStep 86000\tlr: 0.00072\tloss: 1.5699\tElapsed: 1360.2\n",
      "Epoch 1\tStep 87000\tlr: 0.00072\tloss: 1.5701\tElapsed: 1376.0\n",
      "Epoch 1\tStep 88000\tlr: 0.00072\tloss: 1.5741\tElapsed: 1391.8\n",
      "Epoch 1\tStep 89000\tlr: 0.00072\tloss: 1.5771\tElapsed: 1407.7\n",
      "Epoch 1\tStep 90000\tlr: 0.00072\tloss: 1.5800\tElapsed: 1423.5\n",
      "Epoch 1\tStep 91000\tlr: 0.00069\tloss: 1.5829\tElapsed: 1439.3\n",
      "Epoch 1\tStep 92000\tlr: 0.00069\tloss: 1.5830\tElapsed: 1455.1\n",
      "Epoch 1\tStep 93000\tlr: 0.00069\tloss: 1.5818\tElapsed: 1471.0\n",
      "Epoch 2\tStep 94000\tlr: 0.00069\tloss: 1.5731\tElapsed: 1487.0\n",
      "Epoch 2\tStep 95000\tlr: 0.00069\tloss: 1.5059\tElapsed: 1502.8\n",
      "Epoch 2\tStep 96000\tlr: 0.00069\tloss: 1.4636\tElapsed: 1518.7\n",
      "Epoch 2\tStep 97000\tlr: 0.00069\tloss: 1.4444\tElapsed: 1534.6\n",
      "Epoch 2\tStep 98000\tlr: 0.00069\tloss: 1.4202\tElapsed: 1550.4\n",
      "Epoch 2\tStep 99000\tlr: 0.00069\tloss: 1.3992\tElapsed: 1566.3\n",
      "Epoch 2\tStep 100000\tlr: 0.00069\tloss: 1.3796\tElapsed: 1582.1\n",
      "Epoch 2\tStep 101000\tlr: 0.00066\tloss: 1.3938\tElapsed: 1598.0\n",
      "Epoch 2\tStep 102000\tlr: 0.00066\tloss: 1.4037\tElapsed: 1613.8\n",
      "Epoch 2\tStep 103000\tlr: 0.00066\tloss: 1.4119\tElapsed: 1629.6\n",
      "Epoch 2\tStep 104000\tlr: 0.00066\tloss: 1.4150\tElapsed: 1645.4\n",
      "Epoch 2\tStep 105000\tlr: 0.00066\tloss: 1.4316\tElapsed: 1661.2\n",
      "Epoch 2\tStep 106000\tlr: 0.00066\tloss: 1.4387\tElapsed: 1677.0\n",
      "Epoch 2\tStep 107000\tlr: 0.00066\tloss: 1.4456\tElapsed: 1692.8\n",
      "Epoch 2\tStep 108000\tlr: 0.00066\tloss: 1.4495\tElapsed: 1708.6\n",
      "Epoch 2\tStep 109000\tlr: 0.00066\tloss: 1.4468\tElapsed: 1724.4\n",
      "Epoch 2\tStep 110000\tlr: 0.00066\tloss: 1.4435\tElapsed: 1740.2\n",
      "Epoch 2\tStep 111000\tlr: 0.00064\tloss: 1.4473\tElapsed: 1755.9\n",
      "Epoch 2\tStep 112000\tlr: 0.00064\tloss: 1.4489\tElapsed: 1771.7\n",
      "Epoch 2\tStep 113000\tlr: 0.00064\tloss: 1.4489\tElapsed: 1787.4\n",
      "Epoch 2\tStep 114000\tlr: 0.00064\tloss: 1.4548\tElapsed: 1803.2\n",
      "Epoch 2\tStep 115000\tlr: 0.00064\tloss: 1.4597\tElapsed: 1819.0\n",
      "Epoch 2\tStep 116000\tlr: 0.00064\tloss: 1.4634\tElapsed: 1834.8\n",
      "Epoch 2\tStep 117000\tlr: 0.00064\tloss: 1.4652\tElapsed: 1850.5\n",
      "Epoch 2\tStep 118000\tlr: 0.00064\tloss: 1.4673\tElapsed: 1866.2\n",
      "Epoch 2\tStep 119000\tlr: 0.00064\tloss: 1.4722\tElapsed: 1882.0\n",
      "Epoch 2\tStep 120000\tlr: 0.00064\tloss: 1.4748\tElapsed: 1897.8\n",
      "Epoch 2\tStep 121000\tlr: 0.00061\tloss: 1.4758\tElapsed: 1913.7\n",
      "Epoch 2\tStep 122000\tlr: 0.00061\tloss: 1.4763\tElapsed: 1929.5\n",
      "Epoch 2\tStep 123000\tlr: 0.00061\tloss: 1.4772\tElapsed: 1945.3\n",
      "Epoch 2\tStep 124000\tlr: 0.00061\tloss: 1.4763\tElapsed: 1961.1\n",
      "Epoch 2\tStep 125000\tlr: 0.00061\tloss: 1.4736\tElapsed: 1977.0\n",
      "Epoch 2\tStep 126000\tlr: 0.00061\tloss: 1.4739\tElapsed: 1992.8\n",
      "Epoch 2\tStep 127000\tlr: 0.00061\tloss: 1.4753\tElapsed: 2008.7\n",
      "Epoch 2\tStep 128000\tlr: 0.00061\tloss: 1.4778\tElapsed: 2024.5\n",
      "Epoch 2\tStep 129000\tlr: 0.00061\tloss: 1.4798\tElapsed: 2040.3\n",
      "Epoch 2\tStep 130000\tlr: 0.00061\tloss: 1.4816\tElapsed: 2056.2\n",
      "Epoch 2\tStep 131000\tlr: 0.00059\tloss: 1.4835\tElapsed: 2072.1\n",
      "Epoch 2\tStep 132000\tlr: 0.00059\tloss: 1.4848\tElapsed: 2088.0\n",
      "Epoch 2\tStep 133000\tlr: 0.00059\tloss: 1.4864\tElapsed: 2103.8\n",
      "Epoch 2\tStep 134000\tlr: 0.00059\tloss: 1.4871\tElapsed: 2119.6\n",
      "Epoch 2\tStep 135000\tlr: 0.00059\tloss: 1.4918\tElapsed: 2135.4\n",
      "Epoch 2\tStep 136000\tlr: 0.00059\tloss: 1.4955\tElapsed: 2151.2\n",
      "Epoch 2\tStep 137000\tlr: 0.00059\tloss: 1.4987\tElapsed: 2166.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\tStep 138000\tlr: 0.00059\tloss: 1.5021\tElapsed: 2182.7\n",
      "Epoch 2\tStep 139000\tlr: 0.00059\tloss: 1.5026\tElapsed: 2198.5\n",
      "Epoch 2\tStep 140000\tlr: 0.00059\tloss: 1.5017\tElapsed: 2214.3\n",
      "1 epoch elapsed time: 2228.7443907260895\n"
     ]
    }
   ],
   "source": [
    "## training\n",
    "### 위 data feeding에서 in_idx, out_idx 후에 실제 학습을 수행하여, \n",
    "### epoch만큼 학습을 진행한다.\n",
    "tic = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_cost = []\n",
    "    state = [np.zeros([batch_size, rnn_size], dtype=np.float32) for _ in range(layers)]\n",
    "    iters = np.arange(batch_size)\n",
    "    maxiter = iters.max()\n",
    "    \n",
    "    start = offset_sessions[iters]\n",
    "    end = offset_sessions[iters+1]\n",
    "    \n",
    "    finished = False\n",
    "    while not finished:\n",
    "        minlen = (end-start).min()\n",
    "        out_idx = data.ItemIdx.values[start]\n",
    "        for i in range(minlen-1):\n",
    "            in_idx = out_idx\n",
    "            out_idx = data.ItemIdx.values[start+i+1]\n",
    "            # prepare inputs, targeted outputs and hidden states\n",
    "            fetches = [cost, final_state, global_step, lr, train_op]\n",
    "            feed_dict = {X: in_idx, Y: out_idx}\n",
    "            for j in range(layers): \n",
    "                feed_dict[States[j]] = state[j]\n",
    "            \n",
    "            cost_, state, step, lr_, _ = sess.run(fetches, feed_dict)\n",
    "            epoch_cost.append(cost_)\n",
    "                \n",
    "            if step == 1 or step % print_step == 0:\n",
    "                avgc = np.mean(epoch_cost)\n",
    "                print('Epoch {}\\tStep {}\\tlr: {:.5f}\\tloss: {:.4f}\\tElapsed: {:.1f}'.\n",
    "                      format(epoch, step, lr_, avgc, time.time()-tic))\n",
    "\n",
    "        start = start+minlen-1\n",
    "        mask = np.arange(len(iters))[(end-start)<=1]\n",
    "        for idx in mask:\n",
    "            maxiter += 1\n",
    "            if maxiter >= len(offset_sessions)-1:\n",
    "                finished = True\n",
    "                break\n",
    "            iters[idx] = maxiter\n",
    "            start[idx] = offset_sessions[maxiter]\n",
    "            end[idx] = offset_sessions[maxiter+1]\n",
    "        if len(mask):\n",
    "            for i in range(layers):\n",
    "                state[i][mask] = 0\n",
    "        \n",
    "    avgc = np.mean(epoch_cost)\n",
    "    if np.isnan(avgc):\n",
    "        print('Epoch {}: Nan error!'.format(epoch, avgc))\n",
    "        break\n",
    "    saver.save(sess, '{}/gru-model'.format(checkpoint_dir), global_step=epoch)\n",
    "print(\"1 epoch elapsed time:\", time.time() - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Prediction & Evaluation\n",
    "valid(test) 데이터에 대하여 예측과 평가를 수행한다.\n",
    "\n",
    "evaluation metric\n",
    "1. Recall@20\n",
    "    - 예측한 top 20 아이템 중에 정답 아이템이 있는지 1 or 0으로 평가 후 전체를 평균함\n",
    "2. MRR@20 (mean reciprocal rank)\n",
    "    - 정답 아이템의 rank의 역수를 취한 후 전체를 평균함\n",
    "    - $mrr = \\frac{1}{N} \\sum\\limits_{i=1}^{N} {\\frac{1}{rank_{i}}}$\n",
    "    - rank가 20위 안에 들지 않으면 0으로 처리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## parameters\n",
    "cut_off = 20     # @20\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoint/gru-model-2\n"
     ]
    }
   ],
   "source": [
    "## session restore\n",
    "### 마지막(최신) 학습 checkpoint 정보를 restore한다.\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "ckpt = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "saver.restore(sess, ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SessionId</th>\n",
       "      <th>ItemId</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>ItemIdx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11255868</td>\n",
       "      <td>214853754</td>\n",
       "      <td>1.412011e+09</td>\n",
       "      <td>19930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11255868</td>\n",
       "      <td>214577709</td>\n",
       "      <td>1.412011e+09</td>\n",
       "      <td>16686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11255868</td>\n",
       "      <td>214853754</td>\n",
       "      <td>1.412011e+09</td>\n",
       "      <td>19930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11255882</td>\n",
       "      <td>214855046</td>\n",
       "      <td>1.411965e+09</td>\n",
       "      <td>21684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11255882</td>\n",
       "      <td>214854913</td>\n",
       "      <td>1.411965e+09</td>\n",
       "      <td>21665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SessionId     ItemId     timestamp  ItemIdx\n",
       "0   11255868  214853754  1.412011e+09    19930\n",
       "1   11255868  214577709  1.412011e+09    16686\n",
       "2   11255868  214853754  1.412011e+09    19930\n",
       "3   11255882  214855046  1.411965e+09    21684\n",
       "4   11255882  214854913  1.411965e+09    21665"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## valdation data set\n",
    "valid['ItemIdx'] = valid['ItemId'].map(lambda x: itemidmap[x])\n",
    "valid[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  3, 14, 16, 19], dtype=int32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## valid offset sessions\n",
    "### 위 학습과 동일하게 각 세션의 시작점의 index list를 만든다.\n",
    "offset_sessions = np.zeros(valid['SessionId'].nunique()+1, dtype=np.int32)\n",
    "offset_sessions[1:] = valid.groupby('SessionId').size().cumsum()\n",
    "offset_sessions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## init prediction\n",
    "### 예측 세션의 배치 사이즈 보다 작을 경우 배치 사이즈를 조정한다.\n",
    "if len(offset_sessions) - 1 < batch_size:\n",
    "    batch_size = len(offset_sessions) - 1\n",
    "### training step과 동일\n",
    "iters = np.arange(batch_size).astype(np.int32)\n",
    "maxiter = iters.max()\n",
    "start = offset_sessions[iters]\n",
    "end = offset_sessions[iters+1]\n",
    "in_idx = np.zeros(batch_size, dtype=np.int32)\n",
    "predict_state = [np.zeros([batch_size, rnn_size], dtype=np.float32) for _ in range(layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "break at endpoint 5833\n",
      "recall:  0.4923709926281502 mrr: 0.17159312017156844 elapsed time: 1.8133759498596191\n"
     ]
    }
   ],
   "source": [
    "## prediction & evaluation\n",
    "### data feeding 요약\n",
    "### 학습과는 조금 다르게 valid_mask를 설정하여, batch placeholder에 더이상 feed할 세션이 없어지면,\n",
    "### 해당 위치를 꺼가는 방식으로 모든 batch placeholder가 없어질 때까지 feed하는 것이다.\n",
    "evalutation_point_count = 0\n",
    "mrr, recall = 0.0, 0.0\n",
    "tic = time.time()\n",
    "while True:\n",
    "    ### iters는 batch placeholder로 0보다 큰 즉, 마지막 세션까지는 모든 위치를 켜두고\n",
    "    ### 아래에서 session 데이터가 다 소진되면 해당 위치를 -1로 할당할 것이다.\n",
    "    ### valid_mask가 0이 되면 즉 모든 위치가 꺼지면 학습을 종료한다.\n",
    "    valid_mask = iters >= 0\n",
    "    if valid_mask.sum() == 0:\n",
    "        print(\"break at endpoint\", evalutation_point_count)\n",
    "        break\n",
    "        \n",
    "    start_valid = start[valid_mask]\n",
    "    minlen = (end[valid_mask]-start_valid).min()\n",
    "    in_idx[valid_mask] = valid.ItemIdx.values[start_valid]\n",
    "    \n",
    "    for i in range(minlen-1):\n",
    "        out_idx = valid.ItemIdx.values[start_valid+i+1]\n",
    "        ## --- prediction --- ##\n",
    "        fetches = [yhat_all, final_state]\n",
    "        feed_dict = {X: in_idx}\n",
    "        for j in range(layers): \n",
    "            feed_dict[States[j]] = predict_state[j]\n",
    "        preds, predict_state = sess.run(fetches, feed_dict)\n",
    "        preds = pd.DataFrame(data=np.asarray(preds).T)\n",
    "        preds.fillna(0, inplace=True) ### preds shape: (item_size, batch_size)\n",
    "        ## --- evaluation --- ##\n",
    "        in_idx[valid_mask] = out_idx\n",
    "        ### 정답 아이템 prediction 값보다 높은 아이템이 몇개인지 카운트 하여 rank를 계산한다.\n",
    "        ranks = (preds.values.T[valid_mask].T > \n",
    "                 np.diag(preds.loc[in_idx].values)[valid_mask]).sum(axis=0) + 1\n",
    "        ### cutoff에 따른 recall과 mrr을 계산한다.\n",
    "        rank_ok = ranks < cut_off\n",
    "        recall += rank_ok.sum()\n",
    "        mrr += (1.0 / ranks[rank_ok]).sum()\n",
    "        evalutation_point_count += len(ranks)\n",
    "        \n",
    "    start = start+minlen-1\n",
    "    mask = np.arange(len(iters))[(valid_mask) & (end-start<=1)]\n",
    "    \n",
    "    for idx in mask:\n",
    "        maxiter += 1\n",
    "        ## 더 이상 할당할 세션이 없으면 해당 위치에 -1을 할당하여 끈다.\n",
    "        if maxiter >= len(offset_sessions)-1:\n",
    "            iters[idx] = -1\n",
    "        else:\n",
    "            iters[idx] = maxiter\n",
    "            start[idx] = offset_sessions[maxiter]\n",
    "            end[idx] = offset_sessions[maxiter+1]\n",
    "            \n",
    "    if len(mask):\n",
    "        for i in range(layers):\n",
    "            predict_state[i][mask] = 0\n",
    "\n",
    "### 최종 matric을 계산함.\n",
    "recall = recall/evalutation_point_count\n",
    "mrr = mrr/evalutation_point_count\n",
    "print(\"recall: \", recall, \"mrr:\", mrr, \"elapsed time:\", time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5833\n",
      "5833\n"
     ]
    }
   ],
   "source": [
    "### 모든 데이터가 다 사용되었는지 검증.\n",
    "print(evalutation_point_count)\n",
    "print(sum(valid.groupby('SessionId').size() - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
